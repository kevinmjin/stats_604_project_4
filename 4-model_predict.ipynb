{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, pickle, janitor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "import warnings\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "\n",
    "## For the scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCleanDataFrame(folder_path: str):\n",
    "    files = os.listdir(folder_path)\n",
    "    ## keep only the csv files\n",
    "    files = [file for file in files if file.endswith(\".csv\")]\n",
    "    df = pd.concat([pd.read_csv(folder_path + file)\n",
    "                    .pipe(janitor.clean_names, strip_underscores=True)\n",
    "                    .drop(columns=[\"unnamed_0\"])\n",
    "                    .assign(date=lambda x: pd.to_datetime(x.date),\n",
    "                            temp_max=lambda x:pd.to_numeric(x.temp_max),\n",
    "                            temp_min=lambda x:pd.to_numeric(x.temp_min),\n",
    "                            temp_mean=lambda x:pd.to_numeric(x.temp_mean),\n",
    "                            precipitation=lambda x:pd.to_numeric(x.precipitation),\n",
    "                            dew_point_max=lambda x:pd.to_numeric(x.dew_point_max),\n",
    "                            dew_point_min=lambda x:pd.to_numeric(x.dew_point_min),\n",
    "                            dew_point_mean=lambda x:pd.to_numeric(x.dew_point_mean),\n",
    "                            max_wind_speed=lambda x:pd.to_numeric(x.max_wind_speed),\n",
    "                            visibility=lambda x:pd.to_numeric(x.visibility),\n",
    "                            sea_level_pressure=lambda x:pd.to_numeric(x.sea_level_pressure)\n",
    "                    )\n",
    "                     for file in files])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Created Lagged Data\n",
    "def allLaggedDataAvail(df: pd.DataFrame, lag: int, to_lag_columns: list):\n",
    "    df = df.sort_values(by=[\"date\"])\n",
    "    df['day_diff'] = df['date'].diff().dt.days\n",
    "    df['has_all_lagged'] = (\n",
    "        df['day_diff']\n",
    "        .rolling(window=lag)\n",
    "        .apply(lambda x: np.all(x == 1), raw=True)\n",
    "    )\n",
    "\n",
    "\n",
    "    for col in to_lag_columns:\n",
    "        for i_lag in range(1, lag + 1):\n",
    "            df[f'{col}_lag_{i_lag}'] = df[f'{col}'].shift(i_lag)\n",
    "    df = df[df['has_all_lagged'] == 1].copy()\n",
    "    df.dropna(subset=[f'{col}_lag_{lag}' for lag in range(1, lag + 1)], inplace=True)\n",
    "\n",
    "    # Drop temporary columns\n",
    "    df = df.drop(columns=['day_diff', 'has_all_lagged'])\n",
    "    #df = df.drop(columns=to_lag_columns[3:])\n",
    "    return df.sort_values(by = \"date\").reset_index(drop=True)\n",
    "\n",
    "## Create a Model for each Location\n",
    "class LocationModel():\n",
    "    def __init__(self, location: str, df: pd.DataFrame, lag: int):\n",
    "        self.to_lag_columns = ['temp_max',\n",
    "                               'temp_min',\n",
    "                               'temp_mean',\n",
    "                               'precipitation',\n",
    "                               'dew_point_max',\n",
    "                               'dew_point_min',\n",
    "                               'dew_point_mean',\n",
    "                               'max_wind_speed',\n",
    "                               'visibility',\n",
    "                               'sea_level_pressure']\n",
    "        self.lag = lag\n",
    "        \n",
    "        self.location = location\n",
    "        self.df = df.copy()\n",
    "        self.lagged_df = allLaggedDataAvail(df, lag, self.to_lag_columns)\n",
    "\n",
    "    def fit(self):\n",
    "        X_ = (\n",
    "            self.lagged_df\n",
    "            .drop(columns=self.to_lag_columns)\n",
    "            .drop(columns = [\"date\", \"location\"])\n",
    "            .to_numpy()\n",
    "        )\n",
    "        y_ = self.lagged_df[self.to_lag_columns].to_numpy()\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        X_ = self.scaler.fit_transform(X_)\n",
    "        \n",
    "        ## Define the Kernel\n",
    "        gp = GPR(normalize_y=True, n_restarts_optimizer=10)\n",
    "\n",
    "        ## Define the Parameters\n",
    "        param_grid = {\"alpha\": np.logspace(1e-12,10,100),\n",
    "                      \"kernel\": [RBF(l, length_scale_bounds=\"fixed\") for l in np.logspace(-5,5,100)]}\n",
    "        ## Define the Grid Search\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        self.model = GridSearchCV(gp, param_grid=param_grid, cv=tscv, n_jobs=-1, \n",
    "                                  scoring = \"neg_mean_squared_error\").fit(X_, y_)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def predict(self, date, scraped_data = None):\n",
    "        ## If we are including the recent scraped data, append it into our data frame\n",
    "        ## for better predictions\n",
    "        if scraped_data is not None:\n",
    "            bool_vec = np.zeros(len(scraped_data), dtype = bool)\n",
    "            for i in range(len(scraped_data)):\n",
    "                scraped_date = scraped_data.iloc[i][\"date\"]\n",
    "                if scraped_date not in self.df[\"date\"].values:\n",
    "                    bool_vec[i] = True\n",
    "            self.df = pd.concat([self.df, scraped_data[bool_vec]])\n",
    "            self.df = self.df.sort_values(by = \"date\")\n",
    "            \n",
    "        ## check if date is string\n",
    "        if isinstance(date, str):\n",
    "            date = pd.to_datetime(date)\n",
    "        ## check if date is datetime\n",
    "        if not isinstance(date, pd.Timestamp):\n",
    "            raise ValueError(\"Date must be a string or a pd.Timestamp object\") \n",
    "        \n",
    "        ## Clearly, if date is already present in dataframe, we can just return the stored values\n",
    "        if date in self.df[\"date\"].values:\n",
    "            return self.df[self.df.date == date][self.to_lag_columns].to_numpy()\n",
    "        else:\n",
    "            ## check if last lagged days are in the dataframe\n",
    "            prior_date_range = pd.date_range(end = date - pd.Timedelta(days=1), periods = self.lag + 1, freq=\"D\")\n",
    "            for day in prior_date_range:\n",
    "                \n",
    "                if day not in self.df[\"date\"].values:\n",
    "                    _ = self.predict(date=day)\n",
    "                    \n",
    "                    \n",
    "            ## Now, we can predict the values for the date after previous lagged day values are present or predicted\n",
    "            new_X_ = (\n",
    "                self.df[(self.df.date >= prior_date_range[0]) & (self.df.date <= prior_date_range[-1])]\n",
    "                .copy()\n",
    "            )\n",
    "                                    \n",
    "            lagged_new_X = (\n",
    "                allLaggedDataAvail(df = new_X_, lag = self.lag, to_lag_columns = self.to_lag_columns)\n",
    "                .drop(columns = self.to_lag_columns)\n",
    "                .drop(columns = [\"date\", \"location\"])\n",
    "            )            \n",
    "            \n",
    "            try:\n",
    "                predicted_y = self.model.predict(self.scaler.transform(lagged_new_X.to_numpy()))[-1]\n",
    "            except:\n",
    "                ## fill missing values in lagged_new_X with 0\n",
    "                lagged_new_X = lagged_new_X.fillna(0)\n",
    "                predicted_y = self.model.predict(self.scaler.transform(lagged_new_X.to_numpy()))[-1]\n",
    "            \n",
    "            ## clip precipitation to be min 0\n",
    "            if predicted_y[3] < 0:\n",
    "                predicted_y[3] = 0\n",
    "                \n",
    "            ## clip visibility to be max 10\n",
    "            if predicted_y[8] > 10:\n",
    "                predicted_y[8] = 10\n",
    "            \n",
    "            ## \n",
    "            predicted_obs = pd.DataFrame(predicted_y.reshape(1,10), columns = self.to_lag_columns)\n",
    "            predicted_obs[\"date\"] = date\n",
    "            predicted_obs[\"location\"] = self.location\n",
    "            self.df = pd.concat([self.df, predicted_obs])\n",
    "            self.df = self.df.sort_values(by = \"date\").reset_index(drop=True)\n",
    "            return predicted_y\n",
    "                    \n",
    "    def __str__(self):\n",
    "        return f\"{self.location} Model\"\n",
    "    \n",
    "\n",
    "class WeatherForecast():\n",
    "    def __init__(self, folder_path: str, lag: int):\n",
    "        self.df = getCleanDataFrame(folder_path)\n",
    "        self.locations = self.df.location.unique()\n",
    "        self.models = {location: LocationModel(location, self.df[self.df.location == location], lag).fit() for location in self.locations}\n",
    "        \n",
    "    def getCurrenHourlyWeatherData(self, place: str):\n",
    "        col_names = ['date', 'Time', 'wind', 'visibility', 'Weather', 'Sky Condition', 'temp', 'dew_point', 'Max Temp 6hr', \n",
    "                    'Min Temp 6hr', 'Rel Humidity', 'Wind Chill', 'Heat Index', 'sea_level_pressure', 'Other Pressure', \n",
    "                    'precip_1hr', 'Precip 3hr', 'Precip 6hr']\n",
    "        url = f\"https://forecast.weather.gov/data/obhistory/{place}.html\"\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table', class_ = 'obs-history')\n",
    "        for weather_data in table.find_all('tbody'):\n",
    "            rows = weather_data .find_all('tr')\n",
    "            data_dict = {col: [] for col in col_names}\n",
    "            \n",
    "            for row in rows:\n",
    "                for col in col_names:\n",
    "                    data_dict[col].append(row.find_all('td')[col_names.index(col)].text)\n",
    "            data_dict[\"wind\"] = [i.split(\"\\n\")[1].replace(\" \", \"\").replace(\"\\n\", \"\") for i in data_dict[\"wind\"]]\n",
    "            for key in data_dict:\n",
    "                try:\n",
    "                    data_dict[key] = np.array(data_dict[key], dtype = float)\n",
    "                except:\n",
    "                    data_dict[key] = np.array(data_dict[key]) \n",
    "        df = (\n",
    "            pd.DataFrame(data_dict)\n",
    "            .drop(columns = [\"Time\", \n",
    "                             \"Weather\", \n",
    "                             \"Sky Condition\", \n",
    "                             \"Max Temp 6hr\", \n",
    "                             \"Min Temp 6hr\", \n",
    "                             \"Wind Chill\", \n",
    "                             \"Heat Index\", \n",
    "                             \"Other Pressure\", \n",
    "                             \"Precip 3hr\", \n",
    "                             \"Precip 6hr\", \n",
    "                             \"Rel Humidity\"])\n",
    "        )\n",
    "        ## replace missing values in precip_1hr with 0\n",
    "        df[\"precip_1hr\"] = df[\"precip_1hr\"].replace(\"\",0).astype(float)\n",
    "        date_orderings = np.unique(data_dict[\"date\"], return_index = True)\n",
    "        idxs = np.argsort(np.argsort(date_orderings[1]))\n",
    "        \n",
    "        ## for all columns in df, check if there is a missing value, if so, fill it with the previous value\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = df[col].replace(\"\", np.nan).fillna(method = \"ffill\").fillna(0)\n",
    "            except:\n",
    "                print(\"We are goners\")        \n",
    "        \n",
    "        ## necessary just in case the change of date happens....date seems to recorded in weather.gov as just the day number\n",
    "        date_dict = {float(date): int(idxs[i]) for i, date in enumerate(date_orderings[0])}\n",
    "        return df, date_dict\n",
    "\n",
    "\n",
    "    def getCurrentWeatherData(self, place: str):\n",
    "        today = datetime.today()\n",
    "        hourly_df, date_orderings = self.getCurrenHourlyWeatherData(place)\n",
    "        daily_df = (\n",
    "        hourly_df\n",
    "        .groupby('date')\n",
    "        .agg({'temp': ['min', 'max', 'mean'],\n",
    "            'dew_point': ['min', 'max', 'mean'],\n",
    "            'precip_1hr': 'sum',\n",
    "            'sea_level_pressure' : 'mean', \n",
    "            'visibility': 'max',\n",
    "            'wind': \"max\"})\n",
    "        )\n",
    "        daily_df.columns = ['_'.join(col) for col in daily_df.columns]\n",
    "        ## removing grouping of daily_df\n",
    "        daily_df = (\n",
    "            daily_df\n",
    "            .reset_index()\n",
    "            .rename(columns = {'precip_1hr_sum': 'precipitation',\n",
    "                               'sea_level_pressure_mean': 'sea_level_pressure',\n",
    "                               'visibility_max': 'visibility',\n",
    "                               'wind_max': 'max_wind_speed'})\n",
    "        )\n",
    "        \n",
    "        date_range = pd.date_range(periods = len(daily_df), end = today, freq = \"D\")[::-1]\n",
    "        daily_df[\"date\"] = daily_df[\"date\"].apply(lambda x: date_range[date_orderings[x]].floor(\"1d\"))\n",
    "        daily_df[\"location\"] = place\n",
    "        daily_df = daily_df.reindex(columns = ['location', \n",
    "                                               'date', \n",
    "                                               'temp_max', \n",
    "                                               'temp_min', \n",
    "                                               'temp_mean', \n",
    "                                               'precipitation',\n",
    "                                               'dew_point_max',\n",
    "                                               'dew_point_min',\n",
    "                                               'dew_point_mean', \n",
    "                                               'max_wind_speed', \n",
    "                                               'visibility',\n",
    "                                               'sea_level_pressure'])\n",
    "        return daily_df\n",
    "\n",
    "        \n",
    "    def predict_location(self, date, location: str):\n",
    "        try:\n",
    "            current_data = self.getCurrentWeatherData(location)\n",
    "        except:\n",
    "            print(f\"Scraping Failed for {location}\")\n",
    "            current_data = None\n",
    "            \n",
    "        model = self.models[location]\n",
    "        _ = model.predict(date, current_data)\n",
    "        start_date = date - pd.Timedelta(days = 4)\n",
    "        responses = (\n",
    "            model.df[(model.df.date >= start_date) & (model.df.date <= date)]\n",
    "            .copy()[[\"date\", \"temp_min\", \"temp_mean\", \"temp_max\"]]\n",
    "            .reset_index(drop = True)\n",
    "            .sort_values(by = \"date\")        \n",
    "            .drop(columns = \"date\")\n",
    "            .to_numpy()\n",
    "        )\n",
    "        return responses\n",
    "    \n",
    "    def predict_all(self):\n",
    "        today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "        datetime_today = pd.to_datetime(today)\n",
    "        datetime_end = datetime_today + timedelta(days = 5)\n",
    "        \n",
    "        locations = self.locations\n",
    "        #order alphabetically\n",
    "        locations = sorted(locations)\n",
    "        #for location in locations:\n",
    "        predictions = np.zeros((len(locations), 5, 3))\n",
    "        for i, location in enumerate(locations):\n",
    "            predictions[i,:,:] = self.predict_location(datetime_end, location)\n",
    "        predictions = np.round(predictions, 1)\n",
    "        print(f\"{today}, {', '.join(str(ele) for ele in predictions.flatten())}\")\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Failed for KPHX\n",
      "Scraping Failed for PANC\n",
      "2024-11-22, 36.7, 49.0, 62.1, 38.3, 49.5, 61.6, 38.7, 49.9, 62.9, 37.8, 49.1, 63.0, 37.1, 49.3, 62.9, 31.5, 38.7, 48.3, 31.2, 38.4, 47.7, 30.9, 38.0, 47.1, 30.4, 37.6, 47.1, 29.8, 37.1, 46.7, 39.3, 46.5, 54.0, 37.6, 43.9, 50.1, 38.1, 45.2, 52.2, 36.8, 44.3, 52.3, 35.9, 43.1, 50.5, 34.5, 40.9, 48.3, 36.5, 42.9, 49.9, 36.0, 42.3, 49.0, 34.8, 41.2, 48.0, 33.5, 39.5, 45.9, 46.9, 60.1, 74.0, 50.7, 63.3, 76.5, 52.6, 63.2, 74.0, 54.2, 64.6, 75.2, 52.9, 63.0, 73.1, 33.6, 40.5, 46.8, 37.9, 45.3, 52.1, 35.8, 42.7, 49.3, 37.3, 44.5, 51.4, 35.3, 42.0, 48.6, 62.7, 69.8, 77.1, 57.1, 65.9, 74.3, 64.0, 71.9, 79.4, 62.7, 70.5, 78.4, 65.8, 73.2, 80.4, 31.7, 38.8, 46.5, 33.6, 42.0, 49.8, 33.4, 41.1, 48.4, 32.0, 40.0, 47.0, 34.2, 41.5, 48.0, 40.3, 52.4, 66.0, 38.0, 50.4, 64.3, 38.9, 51.2, 65.2, 37.2, 49.3, 62.8, 37.8, 49.8, 63.7, 41.5, 46.2, 51.1, 44.6, 47.9, 52.0, 40.6, 44.2, 49.2, 43.5, 47.6, 52.2, 41.1, 45.1, 50.2, 49.7, 60.4, 71.5, 49.1, 59.9, 71.3, 49.3, 60.0, 71.3, 48.8, 59.6, 71.1, 48.8, 59.4, 70.8, 34.7, 42.0, 49.4, 32.6, 40.5, 47.0, 30.4, 37.1, 43.4, 29.4, 37.1, 44.1, 28.5, 36.2, 42.9, 48.2, 57.9, 68.5, 47.3, 56.0, 65.5, 48.5, 57.2, 67.7, 47.3, 56.3, 66.0, 46.7, 56.3, 66.4, 40.8, 45.4, 50.7, 43.3, 47.1, 52.1, 41.6, 45.5, 50.8, 43.1, 47.2, 52.2, 42.6, 46.5, 51.5, 53.1, 57.2, 62.4, 53.9, 57.8, 63.0, 52.4, 56.6, 61.8, 52.8, 56.8, 61.6, 51.3, 56.6, 61.6, 31.6, 38.8, 47.9, 31.6, 38.8, 47.9, 31.6, 38.8, 47.9, 31.6, 38.8, 47.9, 31.6, 38.8, 47.9, 34.5, 42.5, 51.2, 29.0, 35.7, 42.7, 31.8, 40.3, 49.0, 30.2, 38.2, 46.2, 31.4, 39.7, 47.9, 7.9, 13.9, 19.1, 12.3, 18.1, 22.7, 8.0, 13.8, 18.8, 11.7, 17.4, 22.2, 8.3, 14.0, 19.0, 68.4, 76.4, 83.6, 66.5, 73.3, 79.5, 69.7, 76.2, 82.3, 68.7, 75.7, 82.1, 69.7, 76.2, 82.5\n"
     ]
    }
   ],
   "source": [
    "lag = 5\n",
    "try:\n",
    "    with open(f\"models/WFobj_{lag}.pkl\", \"rb\") as f:\n",
    "        wf = pickle.load(f)\n",
    "except:\n",
    "    print(\"No model found\")\n",
    "predictions = wf.predict_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
