{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, pickle, janitor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "import warnings\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "\n",
    "## For the scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCleanDataFrame(folder_path: str):\n",
    "    files = os.listdir(folder_path)\n",
    "    ## keep only the csv files\n",
    "    files = [file for file in files if file.endswith(\".csv\")]\n",
    "    df = pd.concat([pd.read_csv(folder_path + file)\n",
    "                    .pipe(janitor.clean_names, strip_underscores=True)\n",
    "                    .drop(columns=[\"unnamed_0\"])\n",
    "                    .assign(date=lambda x: pd.to_datetime(x.date),\n",
    "                            temp_max=lambda x:pd.to_numeric(x.temp_max),\n",
    "                            temp_min=lambda x:pd.to_numeric(x.temp_min),\n",
    "                            temp_mean=lambda x:pd.to_numeric(x.temp_mean),\n",
    "                            precipitation=lambda x:pd.to_numeric(x.precipitation),\n",
    "                            dew_point_max=lambda x:pd.to_numeric(x.dew_point_max),\n",
    "                            dew_point_min=lambda x:pd.to_numeric(x.dew_point_min),\n",
    "                            dew_point_mean=lambda x:pd.to_numeric(x.dew_point_mean),\n",
    "                            max_wind_speed=lambda x:pd.to_numeric(x.max_wind_speed),\n",
    "                            visibility=lambda x:pd.to_numeric(x.visibility),\n",
    "                            sea_level_pressure=lambda x:pd.to_numeric(x.sea_level_pressure)\n",
    "                    )\n",
    "                     for file in files])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Created Lagged Data\n",
    "def allLaggedDataAvail(df: pd.DataFrame, lag: int, to_lag_columns: list):\n",
    "    df = df.sort_values(by=[\"date\"])\n",
    "    df['day_diff'] = df['date'].diff().dt.days\n",
    "    df['has_all_lagged'] = (\n",
    "        df['day_diff']\n",
    "        .rolling(window=lag)\n",
    "        .apply(lambda x: np.all(x == 1), raw=True)\n",
    "    )\n",
    "\n",
    "\n",
    "    for col in to_lag_columns:\n",
    "        for i_lag in range(1, lag + 1):\n",
    "            df[f'{col}_lag_{i_lag}'] = df[f'{col}'].shift(i_lag)\n",
    "    df = df[df['has_all_lagged'] == 1].copy()\n",
    "    df.dropna(subset=[f'{col}_lag_{lag}' for lag in range(1, lag + 1)], inplace=True)\n",
    "\n",
    "    # Drop temporary columns\n",
    "    df = df.drop(columns=['day_diff', 'has_all_lagged'])\n",
    "    #df = df.drop(columns=to_lag_columns[3:])\n",
    "    return df.sort_values(by = \"date\").reset_index(drop=True)\n",
    "\n",
    "## Create a Model for each Location\n",
    "class LocationModel():\n",
    "    def __init__(self, location: str, df: pd.DataFrame, lag: int):\n",
    "        self.to_lag_columns = ['temp_max',\n",
    "                               'temp_min',\n",
    "                               'temp_mean',\n",
    "                               'precipitation',\n",
    "                               'dew_point_max',\n",
    "                               'dew_point_min',\n",
    "                               'dew_point_mean',\n",
    "                               'max_wind_speed',\n",
    "                               'visibility',\n",
    "                               'sea_level_pressure']\n",
    "        self.lag = lag\n",
    "        \n",
    "        self.location = location\n",
    "        self.df = df.copy()\n",
    "        self.lagged_df = allLaggedDataAvail(df, lag, self.to_lag_columns)\n",
    "\n",
    "    def fit(self):\n",
    "        X_ = (\n",
    "            self.lagged_df\n",
    "            .drop(columns=self.to_lag_columns)\n",
    "            .drop(columns = [\"date\", \"location\"])\n",
    "            .to_numpy()\n",
    "        )\n",
    "        y_ = self.lagged_df[self.to_lag_columns].to_numpy()\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        X_ = self.scaler.fit_transform(X_)\n",
    "        \n",
    "        ## Define the Kernel\n",
    "        gp = GPR(normalize_y=True, n_restarts_optimizer=10)\n",
    "\n",
    "        ## Define the Parameters\n",
    "        param_grid = {\"alpha\": np.logspace(1e-12,10,100),\n",
    "                      \"kernel\": [RBF(l, length_scale_bounds=\"fixed\") for l in np.logspace(-5,5,100)]}\n",
    "        ## Define the Grid Search\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        self.model = GridSearchCV(gp, param_grid=param_grid, cv=tscv, n_jobs=-1, \n",
    "                                  scoring = \"neg_mean_squared_error\").fit(X_, y_)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def predict(self, date, scraped_data = None):\n",
    "        ## If we are including the recent scraped data, append it into our data frame\n",
    "        ## for better predictions\n",
    "        if scraped_data is not None:\n",
    "            bool_vec = np.zeros(len(scraped_data), dtype = bool)\n",
    "            for i in range(len(scraped_data)):\n",
    "                scraped_date = scraped_data.iloc[i][\"date\"]\n",
    "                if scraped_date not in self.df[\"date\"].values:\n",
    "                    bool_vec[i] = True\n",
    "            self.df = pd.concat([self.df, scraped_data[bool_vec]])\n",
    "            self.df = self.df.sort_values(by = \"date\")\n",
    "            \n",
    "        ## check if date is string\n",
    "        if isinstance(date, str):\n",
    "            date = pd.to_datetime(date)\n",
    "        ## check if date is datetime\n",
    "        if not isinstance(date, pd.Timestamp):\n",
    "            raise ValueError(\"Date must be a string or a pd.Timestamp object\") \n",
    "        \n",
    "        ## Clearly, if date is already present in dataframe, we can just return the stored values\n",
    "        if date in self.df[\"date\"].values:\n",
    "            return self.df[self.df.date == date][self.to_lag_columns].to_numpy()\n",
    "        else:\n",
    "            ## check if last lagged days are in the dataframe\n",
    "            prior_date_range = pd.date_range(end = date - pd.Timedelta(days=1), periods = self.lag + 1, freq=\"D\")\n",
    "            for day in prior_date_range:\n",
    "                \n",
    "                if day not in self.df[\"date\"].values:\n",
    "                    _ = self.predict(date=day)\n",
    "                    \n",
    "                    \n",
    "            ## Now, we can predict the values for the date after previous lagged day values are present or predicted\n",
    "            new_X_ = (\n",
    "                self.df[(self.df.date >= prior_date_range[0]) & (self.df.date <= prior_date_range[-1])]\n",
    "                .copy()\n",
    "            )\n",
    "                                    \n",
    "            lagged_new_X = (\n",
    "                allLaggedDataAvail(df = new_X_, lag = self.lag, to_lag_columns = self.to_lag_columns)\n",
    "                .drop(columns = self.to_lag_columns)\n",
    "                .drop(columns = [\"date\", \"location\"])\n",
    "            )            \n",
    "            \n",
    "            try:\n",
    "                predicted_y = self.model.predict(self.scaler.transform(lagged_new_X.to_numpy()))[-1]\n",
    "            except:\n",
    "                ## fill missing values in lagged_new_X with 0\n",
    "                lagged_new_X = lagged_new_X.fillna(0)\n",
    "                predicted_y = self.model.predict(self.scaler.transform(lagged_new_X.to_numpy()))[-1]\n",
    "            \n",
    "            ## clip precipitation to be min 0\n",
    "            if predicted_y[3] < 0:\n",
    "                predicted_y[3] = 0\n",
    "                \n",
    "            ## clip visibility to be max 10\n",
    "            if predicted_y[8] > 10:\n",
    "                predicted_y[8] = 10\n",
    "            \n",
    "            ## \n",
    "            predicted_obs = pd.DataFrame(predicted_y.reshape(1,10), columns = self.to_lag_columns)\n",
    "            predicted_obs[\"date\"] = date\n",
    "            predicted_obs[\"location\"] = self.location\n",
    "            self.df = pd.concat([self.df, predicted_obs])\n",
    "            self.df = self.df.sort_values(by = \"date\").reset_index(drop=True)\n",
    "            return predicted_y\n",
    "                    \n",
    "    def __str__(self):\n",
    "        return f\"{self.location} Model\"\n",
    "    \n",
    "\n",
    "class WeatherForecast():\n",
    "    def __init__(self, folder_path: str, lag: int):\n",
    "        self.df = getCleanDataFrame(folder_path)\n",
    "        self.locations = self.df.location.unique()\n",
    "        self.models = {location: LocationModel(location, self.df[self.df.location == location], lag).fit() for location in self.locations}\n",
    "        \n",
    "    def getCurrenHourlyWeatherData(self, place: str):\n",
    "        col_names = ['date', 'Time', 'wind', 'visibility', 'Weather', 'Sky Condition', 'temp', 'dew_point', 'Max Temp 6hr', \n",
    "                    'Min Temp 6hr', 'Rel Humidity', 'Wind Chill', 'Heat Index', 'sea_level_pressure', 'Other Pressure', \n",
    "                    'precip_1hr', 'Precip 3hr', 'Precip 6hr']\n",
    "        url = f\"https://forecast.weather.gov/data/obhistory/{place}.html\"\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table', class_ = 'obs-history')\n",
    "        for weather_data in table.find_all('tbody'):\n",
    "            rows = weather_data .find_all('tr')\n",
    "            data_dict = {col: [] for col in col_names}\n",
    "            \n",
    "            for row in rows:\n",
    "                for col in col_names:\n",
    "                    data_dict[col].append(row.find_all('td')[col_names.index(col)].text)\n",
    "            val_list = []\n",
    "            for i in data_dict[\"wind\"]:\n",
    "                val = i.split(\"\\n\")[1].replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "                if val == \"\":\n",
    "                    val = \"0\"\n",
    "                val_list.append(val)\n",
    "            data_dict[\"wind\"] = val_list\n",
    "\n",
    "            for key in data_dict:\n",
    "                try:\n",
    "                    data_dict[key] = np.array(data_dict[key], dtype = float)\n",
    "                except:\n",
    "                    data_dict[key] = np.array(data_dict[key]) \n",
    "        df = (\n",
    "            pd.DataFrame(data_dict)\n",
    "            .drop(columns = [\"Time\", \n",
    "                             \"Weather\", \n",
    "                             \"Sky Condition\", \n",
    "                             \"Max Temp 6hr\", \n",
    "                             \"Min Temp 6hr\", \n",
    "                             \"Wind Chill\", \n",
    "                             \"Heat Index\", \n",
    "                             \"Other Pressure\", \n",
    "                             \"Precip 3hr\", \n",
    "                             \"Precip 6hr\", \n",
    "                             \"Rel Humidity\"])\n",
    "        )\n",
    "        ## replace missing values in precip_1hr with 0\n",
    "        df[\"precip_1hr\"] = df[\"precip_1hr\"].replace(\"\",0).astype(float)\n",
    "        date_orderings = np.unique(data_dict[\"date\"], return_index = True)\n",
    "        idxs = np.argsort(np.argsort(date_orderings[1]))\n",
    "        \n",
    "        ## for all columns in df, check if there is a missing value, if so, fill it with the previous value\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = df[col].replace(\"\", np.nan).fillna(method = \"ffill\").fillna(0)\n",
    "            except:\n",
    "                print(\"We are goners\")        \n",
    "        \n",
    "        ## necessary just in case the change of date happens....date seems to recorded in weather.gov as just the day number\n",
    "        date_dict = {float(date): int(idxs[i]) for i, date in enumerate(date_orderings[0])}\n",
    "        return df, date_dict\n",
    "\n",
    "\n",
    "    def getCurrentWeatherData(self, place: str):\n",
    "        today = datetime.today()\n",
    "        hourly_df, date_orderings = self.getCurrenHourlyWeatherData(place)\n",
    "        daily_df = (\n",
    "        hourly_df\n",
    "        .groupby('date')\n",
    "        .agg({'temp': ['min', 'max', 'mean'],\n",
    "            'dew_point': ['min', 'max', 'mean'],\n",
    "            'precip_1hr': 'sum',\n",
    "            'sea_level_pressure' : 'mean', \n",
    "            'visibility': 'max',\n",
    "            'wind': \"max\"})\n",
    "        )\n",
    "        daily_df.columns = ['_'.join(col) for col in daily_df.columns]\n",
    "        ## removing grouping of daily_df\n",
    "        daily_df = (\n",
    "            daily_df\n",
    "            .reset_index()\n",
    "            .rename(columns = {'precip_1hr_sum': 'precipitation',\n",
    "                               'sea_level_pressure_mean': 'sea_level_pressure',\n",
    "                               'visibility_max': 'visibility',\n",
    "                               'wind_max': 'max_wind_speed'})\n",
    "        )\n",
    "        \n",
    "        date_range = pd.date_range(periods = len(daily_df), end = today, freq = \"D\")[::-1]\n",
    "        daily_df[\"date\"] = daily_df[\"date\"].apply(lambda x: date_range[date_orderings[x]].floor(\"1d\"))\n",
    "        daily_df[\"location\"] = place\n",
    "        daily_df = daily_df.reindex(columns = ['location', \n",
    "                                               'date', \n",
    "                                               'temp_max', \n",
    "                                               'temp_min', \n",
    "                                               'temp_mean', \n",
    "                                               'precipitation',\n",
    "                                               'dew_point_max',\n",
    "                                               'dew_point_min',\n",
    "                                               'dew_point_mean', \n",
    "                                               'max_wind_speed', \n",
    "                                               'visibility',\n",
    "                                               'sea_level_pressure'])\n",
    "        return daily_df\n",
    "\n",
    "        \n",
    "    def predict_location(self, date, location: str):\n",
    "        try:\n",
    "            current_data = self.getCurrentWeatherData(location)\n",
    "        except:\n",
    "            print(f\"Scraping Failed for {location}\")\n",
    "            current_data = None\n",
    "            \n",
    "        model = self.models[location]\n",
    "        _ = model.predict(date, current_data)\n",
    "        start_date = date - pd.Timedelta(days = 4)\n",
    "        responses = (\n",
    "            model.df[(model.df.date >= start_date) & (model.df.date <= date)]\n",
    "            .copy()[[\"date\", \"temp_min\", \"temp_mean\", \"temp_max\"]]\n",
    "            .reset_index(drop = True)\n",
    "            .sort_values(by = \"date\")        \n",
    "            .drop(columns = \"date\")\n",
    "            .to_numpy()\n",
    "        )\n",
    "        return responses\n",
    "    \n",
    "    def predict_all(self):\n",
    "        today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "        datetime_today = pd.to_datetime(today)\n",
    "        datetime_end = datetime_today + timedelta(days = 5)\n",
    "        print(self.locations)\n",
    "        locations_dict = {\n",
    "                \"Anchorage\": \"PANC\",\n",
    "                \"Boise\": \"KBOI\",\n",
    "                \"Chicago\": \"KORD\",\n",
    "                \"Denver\": \"KCOS\",\n",
    "                \"Detroit\": \"KDTW\",\n",
    "                \"Honolulu\": \"PHNL\",\n",
    "                \"Houston\": \"KIAH\",\n",
    "                \"Miami\": \"KMIA\",\n",
    "                \"Minneapolis\": \"KMSP\",\n",
    "                \"Oklahoma City\": \"KOKC\",\n",
    "                \"Nashville\": \"KBNA\",\n",
    "                \"New York\": \"KJFK\",\n",
    "                \"Phoenix\": \"KPHX\",\n",
    "                \"Portland ME\": \"KPWM\",\n",
    "                \"Portland OR\": \"KPDX\",\n",
    "                \"Salt Lake City\": \"KSLC\",\n",
    "                \"San Diego\": \"KSAN\",\n",
    "                \"San Francisco\": \"KSFO\",\n",
    "                \"Seattle\": \"KSEA\",\n",
    "                \"Washington DC\": \"KDCA\"\n",
    "            }\n",
    "            #order alphabetically\n",
    "        locations = sorted(locations_dict.keys())\n",
    "        print(len(locations))\n",
    "        print(len(self.locations))\n",
    "        #for location in locations:\n",
    "        predictions = np.zeros((len(locations), 5, 3))\n",
    "        for i, location in enumerate(locations):\n",
    "            code = locations_dict[location]\n",
    "            try:\n",
    "                predictions[i,:,:] = self.predict_location(datetime_end, code)\n",
    "            except:\n",
    "                print(f\"Failed to predict {location} with code {code}\")\n",
    "        predictions = np.round(predictions, 1)\n",
    "        print(f\"{today}, {', '.join(str(ele) for ele in predictions.flatten())}\")\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KSAN' 'KSFO' 'KSEA' 'KDCA' 'KPHX' 'KPWM' 'KPDX' 'KSLC' 'KMIA' 'KMSP'\n",
      " 'KOKC' 'KBNA' 'KSWF' 'PANC' 'KMDW' 'PHNL' 'KIAH' 'KDTW' 'KBOI']\n",
      "20\n",
      "19\n",
      "Failed to predict Chicago with code KORD\n",
      "Failed to predict Denver with code KDEN\n",
      "Failed to predict New York with code KJFK\n",
      "2024-11-24, 16.1, 20.8, 24.9, 20.1, 24.3, 27.8, 19.2, 23.2, 26.9, 21.1, 25.2, 29.1, 19.5, 23.8, 27.9, 31.6, 38.8, 48.4, 30.9, 38.0, 47.4, 30.8, 37.9, 47.1, 30.3, 37.5, 47.0, 29.3, 36.6, 46.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 37.1, 44.3, 51.6, 36.3, 43.5, 51.3, 36.4, 43.4, 50.7, 35.7, 43.3, 50.8, 35.6, 42.5, 49.5, 68.0, 74.5, 80.7, 66.1, 74.0, 80.5, 68.2, 75.3, 81.5, 68.0, 75.0, 81.4, 68.9, 75.4, 81.7, 52.5, 64.0, 76.4, 56.7, 67.3, 77.9, 51.7, 62.1, 72.5, 55.5, 65.0, 74.1, 51.5, 61.2, 71.5, 59.3, 68.3, 77.0, 62.9, 71.1, 78.9, 64.1, 72.1, 79.6, 65.4, 73.2, 80.6, 65.4, 73.0, 80.3, 34.8, 42.4, 49.8, 33.1, 40.8, 47.6, 34.5, 41.6, 48.1, 33.9, 40.2, 46.5, 35.9, 42.0, 48.4, 38.6, 49.6, 61.4, 40.3, 52.1, 65.7, 38.8, 50.4, 63.9, 40.6, 52.7, 66.5, 39.4, 51.4, 64.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 42.3, 53.7, 66.6, 42.3, 54.2, 67.7, 40.2, 52.4, 66.2, 39.8, 52.0, 65.7, 40.3, 52.0, 64.9, 53.2, 64.4, 76.9, 53.0, 63.8, 75.7, 53.0, 63.4, 74.7, 52.7, 63.1, 74.4, 51.7, 61.8, 72.9, 29.1, 34.8, 40.1, 24.8, 32.5, 38.1, 26.8, 33.7, 40.3, 27.2, 33.7, 40.3, 28.4, 34.7, 41.0, 42.7, 46.5, 51.2, 42.6, 46.4, 50.7, 42.1, 46.0, 50.6, 41.2, 45.7, 50.7, 41.5, 45.5, 50.5, 31.6, 38.8, 47.9, 31.6, 38.8, 47.9, 31.6, 38.8, 47.9, 31.6, 38.8, 47.9, 31.6, 38.8, 47.9, 50.3, 57.4, 64.9, 53.2, 59.7, 66.9, 49.1, 57.9, 67.3, 51.1, 58.7, 67.0, 49.0, 58.3, 68.0, 49.6, 54.5, 60.0, 48.8, 54.2, 60.5, 49.4, 55.0, 61.9, 48.5, 54.4, 60.8, 49.6, 55.0, 61.1, 40.2, 44.2, 49.1, 39.9, 44.3, 49.8, 40.6, 45.1, 49.6, 39.1, 43.6, 48.7, 40.5, 44.6, 49.8, 39.1, 46.0, 52.9, 41.6, 49.6, 57.6, 36.9, 43.9, 50.5, 38.9, 45.9, 52.6, 36.6, 44.1, 51.5\n"
     ]
    }
   ],
   "source": [
    "lag = 5\n",
    "try:\n",
    "    with open(f\"models/WFobj_{lag}.pkl\", \"rb\") as f:\n",
    "        wf = pickle.load(f)\n",
    "except:\n",
    "    print(\"No model found\")\n",
    "predictions = wf.predict_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
